{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:04:58.752069Z","iopub.execute_input":"2024-08-04T04:04:58.752655Z","iopub.status.idle":"2024-08-04T04:05:05.316203Z","shell.execute_reply.started":"2024-08-04T04:04:58.752625Z","shell.execute_reply":"2024-08-04T04:05:05.315173Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset and Data Augmentation","metadata":{}},{"cell_type":"code","source":"# Define transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(), # Convert images to tensors\n    transforms.Normalize((0.5,), (0.5,)) # Normalize the images to [-1, 1]\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:05.318208Z","iopub.execute_input":"2024-08-04T04:05:05.318588Z","iopub.status.idle":"2024-08-04T04:05:05.323131Z","shell.execute_reply.started":"2024-08-04T04:05:05.318563Z","shell.execute_reply":"2024-08-04T04:05:05.322262Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load MNIST training dataset\ntrain_dataset = torchvision.datasets.MNIST(\n    root='./data', train=True, download=True, transform=transform\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:05.324701Z","iopub.execute_input":"2024-08-04T04:05:05.324945Z","iopub.status.idle":"2024-08-04T04:05:08.240029Z","shell.execute_reply.started":"2024-08-04T04:05:05.324923Z","shell.execute_reply":"2024-08-04T04:05:08.239063Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 38968522.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 1067350.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 9352990.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 3707771.27it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load MNIST test dataset\ntest_dataset = torchvision.datasets.MNIST(\n    root='./data', train=False, download=True, transform=transform\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:08.242907Z","iopub.execute_input":"2024-08-04T04:05:08.243402Z","iopub.status.idle":"2024-08-04T04:05:08.257372Z","shell.execute_reply.started":"2024-08-04T04:05:08.243355Z","shell.execute_reply":"2024-08-04T04:05:08.256703Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create DataLoader for training set\ntrain_loader = DataLoader(\n    dataset=train_dataset, batch_size=64, shuffle=True\n)\n\n# Create DataLoader for test set\ntest_loader = DataLoader(\n    dataset=test_dataset, batch_size=64, shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:08.258521Z","iopub.execute_input":"2024-08-04T04:05:08.258850Z","iopub.status.idle":"2024-08-04T04:05:08.263860Z","shell.execute_reply.started":"2024-08-04T04:05:08.258820Z","shell.execute_reply":"2024-08-04T04:05:08.263064Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Modifying ResNet model for MNIST dataset","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nimport torch.nn as nn\n\n# Load the pre-trained ResNet18 model\nmodel = models.resnet18(pretrained=True)\n\n# Modify the first convolutional layer to accept 1 channel (grayscale)\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# Modify the fully connected layer for 10 output classes (MNIST)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 10)\n\n# Move model to the GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:08.265155Z","iopub.execute_input":"2024-08-04T04:05:08.265538Z","iopub.status.idle":"2024-08-04T04:05:09.295144Z","shell.execute_reply.started":"2024-08-04T04:05:08.265509Z","shell.execute_reply":"2024-08-04T04:05:09.293931Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 146MB/s] \n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Setup","metadata":{}},{"cell_type":"code","source":"# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:09.297886Z","iopub.execute_input":"2024-08-04T04:05:09.298729Z","iopub.status.idle":"2024-08-04T04:05:09.304180Z","shell.execute_reply.started":"2024-08-04T04:05:09.298699Z","shell.execute_reply":"2024-08-04T04:05:09.303192Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"# Number of epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    # Print loss for the epoch\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:05:09.305489Z","iopub.execute_input":"2024-08-04T04:05:09.305806Z","iopub.status.idle":"2024-08-04T04:09:40.365490Z","shell.execute_reply.started":"2024-08-04T04:05:09.305779Z","shell.execute_reply":"2024-08-04T04:09:40.364554Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.1790\nEpoch [2/10], Loss: 0.0691\nEpoch [3/10], Loss: 0.0533\nEpoch [4/10], Loss: 0.0451\nEpoch [5/10], Loss: 0.0407\nEpoch [6/10], Loss: 0.0360\nEpoch [7/10], Loss: 0.0278\nEpoch [8/10], Loss: 0.0253\nEpoch [9/10], Loss: 0.0247\nEpoch [10/10], Loss: 0.0203\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing Loop","metadata":{}},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f'Accuracy on the test set: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:09:40.366594Z","iopub.execute_input":"2024-08-04T04:09:40.366919Z","iopub.status.idle":"2024-08-04T04:09:43.311588Z","shell.execute_reply.started":"2024-08-04T04:09:40.366893Z","shell.execute_reply":"2024-08-04T04:09:43.310575Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy on the test set: 98.83%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"code","source":"# Save the trained model\ntorch.save(model.state_dict(), 'mnist_resnet18.pth')\nprint(\"Model saved successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:09:43.314271Z","iopub.execute_input":"2024-08-04T04:09:43.314586Z","iopub.status.idle":"2024-08-04T04:09:43.406841Z","shell.execute_reply.started":"2024-08-04T04:09:43.314559Z","shell.execute_reply":"2024-08-04T04:09:43.405931Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model saved successfully!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PGD Adversarial Attacks onto the Model","metadata":{}},{"cell_type":"code","source":"pip install torchattacks","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:39:00.162300Z","iopub.execute_input":"2024-08-04T04:39:00.163031Z","iopub.status.idle":"2024-08-04T04:39:16.027467Z","shell.execute_reply.started":"2024-08-04T04:39:00.162989Z","shell.execute_reply":"2024-08-04T04:39:16.026343Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting torchattacks\n  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\nRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (2.1.2)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (0.16.2)\nRequirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (1.11.4)\nRequirement already satisfied: tqdm>=4.56.1 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (4.66.4)\nCollecting requests~=2.25.1 (from torchattacks)\n  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: numpy>=1.19.4 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (1.26.4)\nCollecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks) (2024.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (2024.5.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.2->torchattacks) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\nDownloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: idna, chardet, requests, torchattacks\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Uninstalling requests-2.32.3:\n      Successfully uninstalled requests-2.32.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\nconda 24.5.0 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\nconda 24.5.0 requires requests<3,>=2.28.0, but you have requests 2.25.1 which is incompatible.\ndatasets 2.20.0 requires requests>=2.32.2, but you have requests 2.25.1 which is incompatible.\ndocker 7.0.0 requires requests>=2.26.0, but you have requests 2.25.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-server 2.27.2 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires requests>=2.27, but you have requests 2.25.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires requests>=2.27, but you have requests 2.25.1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchattacks\nfrom torchattacks import PGD, CW","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:39:20.600796Z","iopub.execute_input":"2024-08-04T04:39:20.601246Z","iopub.status.idle":"2024-08-04T04:39:20.605655Z","shell.execute_reply.started":"2024-08-04T04:39:20.601217Z","shell.execute_reply":"2024-08-04T04:39:20.604423Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define the model structure\nmodel = models.resnet18()\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 10)\n\n# Load the trained model weights\nmodel.load_state_dict(torch.load('mnist_resnet18.pth'))\nmodel.to(device)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:39:35.899766Z","iopub.execute_input":"2024-08-04T04:39:35.900126Z","iopub.status.idle":"2024-08-04T04:39:36.202402Z","shell.execute_reply.started":"2024-08-04T04:39:35.900096Z","shell.execute_reply":"2024-08-04T04:39:36.201431Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\nimport time\n\n# Define the PGD attack\npgd = torchattacks.PGD(model, eps=0.3, alpha=2/255, steps=40)\n\n# Function to evaluate the model under attack with a progress bar\ndef evaluate_under_attack(loader, model, attack):\n    model.eval()\n    correct = 0\n    total = 0\n    batch_times = []\n    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"Evaluating\")\n    \n    for i, (inputs, labels) in progress_bar:\n        start_time = time.time()\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Enable gradients for the inputs\n        inputs.requires_grad = True\n        \n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Disable gradients for the inputs\n        inputs.requires_grad = False\n\n        batch_time = time.time() - start_time\n        batch_times.append(batch_time)\n        \n        # Update progress bar with batch accuracy and average batch time\n        progress_bar.set_postfix(batch_accuracy=(correct / total), avg_batch_time=sum(batch_times) / len(batch_times))\n    \n    accuracy = correct / total\n    return accuracy\n\n# Evaluate the model under PGD attack\nadv_accuracy = evaluate_under_attack(test_loader, model, pgd)\nprint(f'Adversarial Accuracy under PGD Attack: {adv_accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:56:08.829430Z","iopub.execute_input":"2024-08-04T04:56:08.830327Z","iopub.status.idle":"2024-08-04T04:57:12.035602Z","shell.execute_reply.started":"2024-08-04T04:56:08.830289Z","shell.execute_reply":"2024-08-04T04:57:12.034451Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 157/157 [01:03<00:00,  2.48it/s, avg_batch_time=0.386, batch_accuracy=0.304]","output_type":"stream"},{"name":"stdout","text":"Adversarial Accuracy under PGD Attack: 30.42%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Carlini Wagner Attack","metadata":{}},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\nimport time\n\n# Define the Carlini-Wagner attack\ncw = torchattacks.CW(model, c=1e-4, kappa=0, steps=1000, lr=0.01)\n\n# Function to evaluate the model under CW attack with a progress bar\ndef evaluate_under_attack(loader, model, attack):\n    model.eval()\n    correct = 0\n    total = 0\n    batch_times = []\n    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"Evaluating\")\n    \n    for i, (inputs, labels) in progress_bar:\n        start_time = time.time()\n        \n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Enable gradients for the inputs\n        inputs.requires_grad = True\n        \n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Disable gradients for the inputs\n        inputs.requires_grad = False\n\n        batch_time = time.time() - start_time\n        batch_times.append(batch_time)\n        \n        # Update progress bar with batch accuracy and average batch time\n        progress_bar.set_postfix(batch_accuracy=(correct / total), avg_batch_time=sum(batch_times) / len(batch_times))\n    \n    accuracy = correct / total\n    return accuracy\n\n# Evaluate the model under CW attack\ncw_accuracy = evaluate_under_attack(test_loader, model, cw)\nprint(f'Adversarial Accuracy under CW Attack: {cw_accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:59:02.157241Z","iopub.execute_input":"2024-08-04T04:59:02.157856Z","iopub.status.idle":"2024-08-04T05:25:42.937942Z","shell.execute_reply.started":"2024-08-04T04:59:02.157826Z","shell.execute_reply":"2024-08-04T05:25:42.937064Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 157/157 [26:40<00:00, 10.20s/it, avg_batch_time=10.2, batch_accuracy=0.774]","output_type":"stream"},{"name":"stdout","text":"Adversarial Accuracy under CW Attack: 77.42%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}